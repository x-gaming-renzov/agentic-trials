{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nodes.understand_element_nodes import agent_preprocessor, agent_code_generation, agent_extract_code, agent_code_review\n",
    "from src.nodes.understand_element_nodes import conditional_should_continue_after_extraction, conditional_should_continue_after_code_review\n",
    "import os, io, json, docker, tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class AgentState(TypedDict):\n",
    "    file_path: str\n",
    "    first_few_lines: str\n",
    "    user_info_about_data: str\n",
    "    first_few_elements_path_name: str\n",
    "    extracted_python_code: str\n",
    "    code_review_result: CodeReviewResult\n",
    "    generated_code: str\n",
    "    instructions_to_code: str\n",
    "    code_extraction_status: str\n",
    "    code_review_status: str\"\"\"\n",
    "\n",
    "init_state = {\n",
    "    \"file_path\": \"data.json\",\n",
    "    \"first_few_lines\": \"\",\n",
    "    \"user_info_about_data\": \"\",\n",
    "    \"first_few_elements_path_name\": \"elements.json\",\n",
    "    \"extracted_python_code\": \"\",\n",
    "    \"code_review_result\": \"\",\n",
    "    \"generated_code\": \"\",\n",
    "    \"instructions_to_code\": \"\",\n",
    "    \"code_extraction_status\": \"\",\n",
    "    \"code_review_status\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.json is very large, so only first few lines are loaded. load only 10% of the file\n",
    "def load_data(file_path):\n",
    "    with open('data.json', 'r') as file:\n",
    "        chunk = file.read(1024)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state[\"first_few_lines\"] = load_data(init_state[\"file_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state[\"user_info_about_data\"] = \"This is a json file. It contains a list of dictionaries. Elements of list are minecraft events triggered by players in minecraft server.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = agent_preprocessor(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = init_state.copy()\n",
    "temp = agent_code_generation(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = init_state.copy()\n",
    "temp = agent_extract_code(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = temp\n",
    "temp = init_state.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = agent_code_review(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_docker(init_state):\n",
    "\n",
    "    client = docker.from_env()\n",
    "\n",
    "    # Use a safer temporary directory\n",
    "    custom_temp_base_dir = \"temp\"\n",
    "    os.makedirs(custom_temp_base_dir, exist_ok=True)\n",
    "    temp_dir = tempfile.mkdtemp(dir=custom_temp_base_dir)\n",
    "    print(f\"Temporary directory created at: {temp_dir}\")\n",
    "\n",
    "    container = None\n",
    "    json_data = None\n",
    "\n",
    "    # Create data.json\n",
    "    data_json_path = init_state['file_path']\n",
    "    destination_data_path = os.path.join(temp_dir, \"data.json\")\n",
    "    shutil.copy(data_json_path, destination_data_path)\n",
    "    print(f\"Copied data.json to: {destination_data_path}\")\n",
    "\n",
    "    \"\"\"# Create elements.json as an empty file\n",
    "    destination_elements_path = os.path.join(temp_dir, \"elements.json\")\n",
    "    with open(destination_elements_path, \"w\") as f:\n",
    "        json.dump([], f)\n",
    "    print(f\"Created empty elements.json at: {destination_elements_path}\")\"\"\"\n",
    "\n",
    "    # Write the Python script\n",
    "    temp_script_path = os.path.join(temp_dir, \"temp_script.py\")\n",
    "    with open(temp_script_path, \"w\") as f:\n",
    "        f.write(init_state['extracted_python_code'])\n",
    "    print(f\"Written script to: {temp_script_path}\")\n",
    "\n",
    "    # Run the Docker container\n",
    "    container = client.containers.run(\n",
    "        image=\"python\",\n",
    "        volumes={os.path.abspath(temp_dir): {'bind': '/usr/src/app', 'mode': 'rw', 'size': '5g'}},\n",
    "        working_dir=\"/usr/src/app\",\n",
    "        detach=True,\n",
    "        mem_limit=\"5g\",\n",
    "        stdout=True,\n",
    "        stderr=True\n",
    "    )\n",
    "    container = client.containers.get(container.id)\n",
    "    print(f\"Container started: {container.id}\")\n",
    "    exit_code, output = container.exec_run(f\"python temp_script.py\", workdir=\"/usr/src/app\", privileged=True)\n",
    "    print(\"Python script output:\", output.decode(\"utf-8\"))\n",
    "    \n",
    "    # Debug: List files in the container\n",
    "    con\n",
    "    exit_code, output = container.exec_run(\"ls -l /usr/src/app\", privileged=True)\n",
    "    print(\"Files in container:\", output.decode(\"utf-8\"))\n",
    "\n",
    "    # Retrieve elements.json\n",
    "    output_file_path = \"/usr/src/app/elements.json\"\n",
    "    tar_stream, _ = container.get_archive(output_file_path)\n",
    "    tar_bytes = io.BytesIO(b\"\".join(tar_stream))\n",
    "\n",
    "    print(\"Extracted TAR file:\", tar_stream)\n",
    "\n",
    "    # Extract JSON from TAR\n",
    "    with tarfile.open(fileobj=tar_bytes, mode=\"r\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.name.endswith(\".json\"):\n",
    "                json_file = tar.extractfile(member)\n",
    "                json_data = json.loads(json_file.read().decode(\"utf-8\"))\n",
    "                print(\"Extracted JSON data:\", json_data)\n",
    "\n",
    "    return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_with_docker(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_temp_base_dir = \"temp\"\n",
    "os.makedirs(custom_temp_base_dir, exist_ok=True)\n",
    "temp_dir = tempfile.mkdtemp(dir=custom_temp_base_dir)\n",
    "print(f\"Temporary directory created at: {temp_dir}\")\n",
    "\n",
    "container = None\n",
    "json_data = None\n",
    "\n",
    "# Create data.json\n",
    "data_json_path = init_state['file_path']\n",
    "destination_data_path = os.path.join(temp_dir, \"data.json\")\n",
    "shutil.copy(data_json_path, destination_data_path)\n",
    "print(f\"Copied data.json to: {destination_data_path}\")\n",
    "\n",
    "# Create elements.json as an empty file\n",
    "destination_elements_path = os.path.join(temp_dir, \"elements.json\")\n",
    "with open(destination_elements_path, \"w\") as f:\n",
    "    json.dump([], f)\n",
    "print(f\"Created empty elements.json at: {destination_elements_path}\")\n",
    "\n",
    "# Write the Python script\n",
    "temp_script_path = os.path.join(temp_dir, \"temp_script.py\")\n",
    "with open(temp_script_path, \"w\") as f:\n",
    "    f.write(init_state['extracted_python_code'])\n",
    "print(f\"Written script to: {temp_script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = docker.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a container with temporary directory as volume\n",
    "container = client.containers.run(\n",
    "    image=\"python:3.9-slim\",\n",
    "    volumes={temp_dir: {'bind': '/usr/src/app', 'mode': 'rw'}},\n",
    "    working_dir=\"/usr/src/app\",\n",
    "    detach=True,\n",
    "    mem_limit=\"6g\",\n",
    "    command=\"python temp_script.py\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import CodeInterpreterTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = CodeInterpreterTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool.run_code_in_docker(code=\"\"\"\n",
    "import json\n",
    "import os\n",
    "\n",
    "def extract_and_save_elements(input_filename, output_filename, num_elements):\n",
    "    base_path = os.getcwd()\n",
    "    input_file_path = os.path.join(base_path, input_filename)\n",
    "    output_file_path = os.path.join(base_path, output_filename)\n",
    "\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    extracted_elements = data[:num_elements]\n",
    "\n",
    "    print(extracted_elements)\n",
    "\n",
    "extract_and_save_elements('data.json', 'elements.json', 5)\n",
    "\"\"\", libraries_used=['json', 'os'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp/data.json', 'r') as file:\n",
    "    elements = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def flatten_json(nested_json, parent_key='', sep='.'):\n",
    "    \"\"\"\n",
    "    Flatten a nested JSON dictionary while preserving semantic field names.\n",
    "    \"\"\"\n",
    "    flattened = {}\n",
    "    for key, value in nested_json.items():\n",
    "        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            # Recurse into nested dictionaries\n",
    "            flattened.update(flatten_json(value, new_key, sep))\n",
    "        elif isinstance(value, list):\n",
    "            # Handle lists: Join as comma-separated values for strings, keep as-is otherwise\n",
    "            flattened[new_key] = ', '.join(map(str, value)) if all(isinstance(i, str) for i in value) else value\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "def flatten_large_json(json_obj, limit=100):\n",
    "    \"\"\"\n",
    "    Flatten a large JSON object with events at the top level and limit the output to the first 'limit' events.\n",
    "    \"\"\"\n",
    "    flattened_list = []\n",
    "    count = 0\n",
    "    for event_id, event_data in json_obj.items():\n",
    "        if count >= limit:\n",
    "            break\n",
    "        # Flatten each event and add the event ID as a field\n",
    "        flattened_event = flatten_json(event_data)\n",
    "        flattened_event['event.id'] = event_id\n",
    "        flattened_list.append(flattened_event)\n",
    "        count += 1\n",
    "    return flattened_list\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # Load the JSON file (replace with your file path for the large JSON)\n",
    "    with open(\"temp/data.json\", \"r\") as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Flatten the JSON and limit to 100 player events\n",
    "    flattened_data = flatten_large_json(json_data, limit=50)\n",
    "\n",
    "    # Save the output to a file or print\n",
    "    with open(\"flattened_events.json\", \"w\") as output_file:\n",
    "        json.dump(flattened_data, output_file, indent=2)\n",
    "\n",
    "    # Optional: Print a few examples\n",
    "    for event in flattened_data[:5]:  # Print first 5 for inspection\n",
    "        print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event_type': 'block_break', 'player_id': 'player_6254', 'timestamp': '2024-11-19T23:37:22Z', 'location.x': -27, 'location.y': 63, 'location.z': -36, 'details.block_type': 'sand', 'details.tool_used': 'wooden_axe', 'details.dropped_items': 'stone', 'event.id': 'event_001'}\n",
      "{'event_type': 'player_join', 'player_id': 'player_2731', 'timestamp': '2024-11-19T08:30:46Z', 'location.x': 218, 'location.y': 60, 'location.z': -238, 'details.message': 'Player joined the game', 'event.id': 'event_002'}\n",
      "{'event_type': 'player_death', 'player_id': 'player_2195', 'timestamp': '2024-11-12T12:47:27Z', 'location.x': -478, 'location.y': 60, 'location.z': 495, 'details.cause': 'drowned', 'details.killer': 'player_1863', 'details.items_dropped': 'iron_sword, sand', 'event.id': 'event_003'}\n",
      "{'event_type': 'chat_message', 'player_id': 'player_4048', 'timestamp': '2024-11-20T06:13:17Z', 'location.x': -498, 'location.y': 63, 'location.z': 248, 'details.message': 'Does anyone have spare food?', 'details.chat_channel': 'global', 'event.id': 'event_004'}\n",
      "{'event_type': 'player_join', 'player_id': 'player_7136', 'timestamp': '2024-11-18T01:53:53Z', 'location.x': 228, 'location.y': 61, 'location.z': -52, 'details.message': 'Player joined the game', 'event.id': 'event_005'}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"flattened_events.json\", \"r\") as file:\n",
    "    flattened_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all unique fields in a flattened json\n",
    "unique_fields = set()\n",
    "for event in flattened_data:\n",
    "    unique_fields.update(event.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.states.understand_element_state import FieldInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_element_with_field_name(field_name):\n",
    "    count = 0\n",
    "    elements = []\n",
    "    for event in flattened_data:\n",
    "        if field_name in event:\n",
    "            count += 1\n",
    "            elements.append(event)\n",
    "            if count >= 5:\n",
    "                break\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_info_list = []\n",
    "for field_name in unique_fields:\n",
    "    field_elements = get_element_with_field_name(field_name)\n",
    "    field_data_type = type(field_elements[0][field_name]).__name__\n",
    "    field_values = [event[field_name] for event in field_elements]\n",
    "\n",
    "    #check if field_data types can be very large ie list, str, bytes, set, tuple etc\n",
    "    if field_data_type in ['list', 'set', 'tuple', 'dict', 'bytes', 'str', 'bytearray']:\n",
    "        #convert to string and check if larger than 1000 characters\n",
    "        temp_field_values = [str(value) for value in field_values]\n",
    "        if any(len(value) > 1000 for value in temp_field_values):\n",
    "            #save only first 1000 characters as str leaving field_data_type unchanged\n",
    "            field_values = [value[:1000] for value in temp_field_values]\n",
    "    \n",
    "    #check if and field values in field_elements are of type list, set, tuple, dict, bytes, str, bytearray\n",
    "    for element in field_elements:\n",
    "        for element_field_name, element_field_value in element.items():\n",
    "            if type(element_field_value).__name__ in ['list', 'set', 'tuple', 'dict', 'bytes', 'str', 'bytearray']:\n",
    "                #convert to string and check if larger than 1000 characters\n",
    "                if len(str(element_field_value)) > 1000:\n",
    "                    element[element_field_name] = str(element_field_value)[:1000]\n",
    "\n",
    "    field_info = {\n",
    "        \"field_name\": field_name,\n",
    "        \"field_value_info\" : {\n",
    "            \"data_type\": field_data_type,\n",
    "            \"example_values\": field_values\n",
    "        },\n",
    "        \"field_elements\": field_elements \n",
    "    }\n",
    "    field_info_list.append(field_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"field_info.json\", \"w\") as file:\n",
    "    json.dump(field_info_list, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'corrected_field_names_spelling_list' from 'src.states.understand_element_state' (/Volumes/RenzovPersonal/packages/LiveOpsFrontDashboardMain/backend-testing/agentic-trials/src/states/understand_element_state.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstates\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munderstand_element_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corrected_field_names_spelling_list\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m), streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m correct_field_name_spelling \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwith_structured_output(corrected_field_names_spelling_list)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'corrected_field_names_spelling_list' from 'src.states.understand_element_state' (/Volumes/RenzovPersonal/packages/LiveOpsFrontDashboardMain/backend-testing/agentic-trials/src/states/understand_element_state.py)"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from src.states.understand_element_state import corrected_field_names_spelling_list\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", api_key=os.getenv(\"OPENAI_API_KEY\"), streaming=True)\n",
    "\n",
    "correct_field_name_spelling = model.with_structured_output(corrected_field_names_spelling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
